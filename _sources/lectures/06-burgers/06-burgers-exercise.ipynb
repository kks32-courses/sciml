{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfmWuid94DU_"
      },
      "source": [
        "# 06: Forward and inverse modeling of Burger's Equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/06-burgers/06-burgers-exercise.ipynb)\n",
        "**Solution:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/06-burgers/06-burgers.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y61YA90-WIZ1"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZrPCr9GWEAJ",
        "outputId": "7fda1941-4085-4949-fa47-9db253448c06"
      },
      "outputs": [],
      "source": [
        "#Latin Hypercube Sampling\n",
        "!pip3 install pyDOE  --quiet\n",
        "!pip3 install urllib3 --quiet\n",
        "!pip3 install scipy --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OdYMzuSDi7Js",
        "outputId": "2c11883b-adef-4f57-d566-17dcee2b04b7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.autograd as autograd         # computation graph\n",
        "import torch.nn as nn                     # neural networks\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from pyDOE import lhs         #Latin Hypercube Sampling\n",
        "import scipy.io\n",
        "\n",
        "#Set default dtype to float32\n",
        "torch.set_default_dtype(torch.float)\n",
        "\n",
        "#PyTorch random number generator\n",
        "torch.manual_seed(1234)\n",
        "\n",
        "# Random number generators in other libraries\n",
        "np.random.seed(1234)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(device)\n",
        "\n",
        "if device == 'cuda':\n",
        "    print(torch.cuda.get_device_name())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqCzgLSp61M"
      },
      "source": [
        "### Tunning Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2r-CxAnp5Ub"
      },
      "outputs": [],
      "source": [
        "steps=10000\n",
        "lr=1e-1\n",
        "layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers\n",
        "# Nu: Number of training points\n",
        "N_u = 100       #Total number of data points for 'u'\n",
        "# Nf: Number of collocation points (Evaluate PDE)\n",
        "N_f = 10000     # Total number of collocation points\n",
        "nu = 0.01/np.pi # diffusion coeficient"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0j73GoH86-m"
      },
      "source": [
        "### Auxiliary Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8Y6q7m288zJ"
      },
      "outputs": [],
      "source": [
        "def plot3D(x,t,y):\n",
        "  x_plot =x.squeeze(1)\n",
        "  t_plot =t.squeeze(1)\n",
        "  X,T= torch.meshgrid(x_plot,t_plot)\n",
        "  F_xt = y\n",
        "  fig,ax=plt.subplots(1,1)\n",
        "  cp = ax.contourf(T,X, F_xt,20,cmap=\"rainbow\")\n",
        "  fig.colorbar(cp) # Add a colorbar to a plot\n",
        "  ax.set_title('F(x,t)')\n",
        "  ax.set_xlabel('t')\n",
        "  ax.set_ylabel('x')\n",
        "  plt.show()\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_surface(T.numpy(), X.numpy(), F_xt.numpy(),cmap=\"rainbow\")\n",
        "  ax.set_xlabel('t')\n",
        "  ax.set_ylabel('x')\n",
        "  ax.set_zlabel('f(x,t)')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFS1eS92dhuX"
      },
      "outputs": [],
      "source": [
        "def plot3D_Matrix(x,t,y):\n",
        "  X,T= x,t\n",
        "  F_xt = y\n",
        "  fig,ax=plt.subplots(1,1)\n",
        "  cp = ax.contourf(T,X, F_xt,20,cmap=\"rainbow\")\n",
        "  fig.colorbar(cp) # Add a colorbar to a plot\n",
        "  ax.set_title('F(x,t)')\n",
        "  ax.set_xlabel('t')\n",
        "  ax.set_ylabel('x')\n",
        "  plt.show()\n",
        "  ax = plt.axes(projection='3d')\n",
        "  ax.plot_surface(T.numpy(), X.numpy(), F_xt.numpy(),cmap=\"rainbow\")\n",
        "  ax.set_xlabel('t')\n",
        "  ax.set_ylabel('x')\n",
        "  ax.set_zlabel('f(x,t)')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7m02X_26hFk"
      },
      "outputs": [],
      "source": [
        "def solutionplot(u_pred,X_u_train,u_train):\n",
        "    # https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.axis('off')\n",
        "\n",
        "    gs0 = gridspec.GridSpec(1, 2)\n",
        "    gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "    ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "    h = ax.imshow(u_pred, interpolation='nearest', cmap='rainbow',\n",
        "                extent=[T.min(), T.max(), X.min(), X.max()],\n",
        "                origin='lower', aspect='auto')\n",
        "    divider = make_axes_locatable(ax)\n",
        "    cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "    fig.colorbar(h, cax=cax)\n",
        "\n",
        "    ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "    line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "    ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "    ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
        "\n",
        "    ax.set_xlabel('$t$')\n",
        "    ax.set_ylabel('$x$')\n",
        "    ax.legend(frameon=False, loc = 'best')\n",
        "    ax.set_title('$u(x,t)$', fontsize = 10)\n",
        "\n",
        "    # Slices of the solution at points t = 0.25, t = 0.50 and t = 0.75\n",
        "    \n",
        "    ####### Row 1: u(t,x) slices ##################\n",
        "    gs1 = gridspec.GridSpec(1, 3)\n",
        "    gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 0])\n",
        "    ax.plot(x,usol.T[25,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,u_pred.T[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(x,t)$')\n",
        "    ax.set_title('$t = 0.25s$', fontsize = 10)\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 1])\n",
        "    ax.plot(x,usol.T[50,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,u_pred.T[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(x,t)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.50s$', fontsize = 10)\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "    ax = plt.subplot(gs1[0, 2])\n",
        "    ax.plot(x,usol.T[75,:], 'b-', linewidth = 2, label = 'Exact')\n",
        "    ax.plot(x,u_pred.T[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
        "    ax.set_xlabel('$x$')\n",
        "    ax.set_ylabel('$u(x,t)$')\n",
        "    ax.axis('square')\n",
        "    ax.set_xlim([-1.1,1.1])\n",
        "    ax.set_ylim([-1.1,1.1])\n",
        "    ax.set_title('$t = 0.75s$', fontsize = 10)\n",
        "\n",
        "    plt.savefig('Burgers.png',dpi = 500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koofMp-5Y-UA"
      },
      "source": [
        "# Foward solution to Burgers' Equation\n",
        "\n",
        "Burgers' equation is a fundamental partial differential equation (PDE) in fluid dynamics and gas dynamics. It combines nonlinear advection and linear diffusion, making it a useful equation for studying the interaction of these two processes.\n",
        "\n",
        "The one-dimensional, unsteady, viscous Burgers' equation is given by:\n",
        "\n",
        "$$ \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = \\nu \\frac{\\partial^2 u}{\\partial x^2} $$\n",
        "\n",
        "Where:\n",
        "- $ u = u(x,t) $ is the dependent variable (e.g., velocity in fluid dynamics).\n",
        "- $ x\\in[-1,1] $ is the spatial coordinate.\n",
        "- $ t\\in[0,1] $ is time.\n",
        "- $ \\nu $ is the kinematic viscosity, which determines the strength of the diffusive term. It's a positive constant.\n",
        "\n",
        "There are two main terms in this equation:\n",
        "1. **Nonlinear Advection Term**: $ u \\frac{\\partial u}{\\partial x} $ – This term represents the nonlinear convective transport of the quantity $ u $. The nonlinearity arises due to the product $ u \\cdot \\frac{\\partial u}{\\partial x} $, which can lead to the formation of shock waves or steep gradients in the solution, especially in the inviscid limit ($ \\nu \\to 0 $).\n",
        "  \n",
        "2. **Diffusion Term**: $ \\nu \\frac{\\partial^2 u}{\\partial x^2} $ – This term acts to smooth out sharp gradients or shocks in the solution. For higher values of $ \\nu $, diffusion dominates, and the solution will be smoother.\n",
        "\n",
        "When $ \\nu = 0 $, the viscous term drops out, and the equation becomes the inviscid Burgers' equation:\n",
        "\n",
        "$$ \\frac{\\partial u}{\\partial t} + u \\frac{\\partial u}{\\partial x} = 0 $$\n",
        "\n",
        "This inviscid version is hyperbolic and can develop discontinuities (shocks) from smooth initial conditions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYwWBwmbp3S6"
      },
      "source": [
        "## PDE\n",
        "\n",
        "Let:\n",
        "\n",
        "$u_t=\\frac{\\partial u}{\\partial t}$\n",
        "\n",
        "$u_x=\\frac{\\partial u}{\\partial x}$\n",
        "\n",
        "$u_{xx}=\\frac{\\partial^2 u}{\\partial x^2}$\n",
        "\n",
        "So:\n",
        "\n",
        "$$u_t+u(u_x)=\\nu u_{xx}$$\n",
        "\n",
        "If we rearrange our PDE, we get:\n",
        "\n",
        "$$u_t+u(u_x)-\\nu u_{xx}=0$$\n",
        "\n",
        "## Neural Network\n",
        "\n",
        "A Neural Network is a function:\n",
        "\n",
        "$$NN(X)=W_n\\sigma_{n-1}(W_{n-1}\\sigma_{n-2}(...(W_2(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n$$\n",
        "\n",
        "> Note: We usually train our NN by iteratively minimizing a loss function ($MSE$:mean squared error) in the training dataset(known data).\n",
        "\n",
        "## PINNs=Neural Network + PDE\n",
        "(See: https://www.sciencedirect.com/science/article/pii/S0021999118307125)\n",
        "\n",
        "We can use a neural network to approximate any function (Universal APproximation Theorem): (See:https://book.sciml.ai/notes/03/)\n",
        "$$NN(x,t)\\approx u(x,t)$$\n",
        "\n",
        "Since NN is a function, we can obtain its derivatives: $\\frac{\\partial NN}{\\partial t},\\frac{\\partial^2 NN}{\\partial x^2}$.(Automatic Diferentiation)\n",
        "\n",
        "Assume:$$NN(t,x)\\approx u(t,x)$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$\\frac{\\partial NN}{\\partial t}+N(\\frac{\\partial NN}{\\partial x})-\\nu\\frac{\\partial^2 NN}{\\partial x^2}\\approx u_t+u(u_x)-\\nu u_{xx}=0$$\n",
        "\n",
        "And:\n",
        "\n",
        "$$\\frac{\\partial NN}{\\partial t}+N(\\frac{\\partial NN}{\\partial x})-\\nu\\frac{\\partial^2 NN}{\\partial x^2}\\approx 0$$\n",
        "\n",
        "\n",
        "We define this function as $f$:\n",
        "\n",
        "$$f(t,x)=\\frac{\\partial NN}{\\partial t}+N(\\frac{\\partial NN}{\\partial x})-\\nu\\frac{\\partial^2 NN}{\\partial x^2}$$\n",
        "\n",
        "If $f\\rightarrow 0$ then our NN would be respecting the physical law.\n",
        "\n",
        "### PINNs' Loss function\n",
        "\n",
        "We evaluate our PDE in a certain number of \"collocation points\" ($N_f$) inside our domain $(x,t)$. Then we iteratively minimize a loss function related to $f$:\n",
        "\n",
        "$$MSE_f=\\frac{1}{N_f}\\sum^{N_f}_{i=1}|f(t_f^i,x_f^i)|^2$$\n",
        "\n",
        "Usually, the training data set is a set of points from which we know the answer. In our case, we will use our boundary(BC) and initial conditions(IC).\n",
        "\n",
        "Since we know the outcome, we select $N_u$ points from our BC and IC and used them to train our network.\n",
        "\n",
        "$$MSE_{u}=\\frac{1}{N_u}\\sum^{N_u}_{i=1}|y(t_{u}^i,x_u^i)-NN(t_{u}^i,x_u^i)|^2$$\n",
        "\n",
        "\n",
        "#### Total Loss:\n",
        "\n",
        "$$MSE=MSE_{u}+MSE_f$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHYqtyYJs3Jv"
      },
      "source": [
        "### Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7EZjlbVMi8w0"
      },
      "outputs": [],
      "source": [
        "class FCN(nn.Module):\n",
        "\n",
        "    def __init__(self,layers):\n",
        "        super().__init__()\n",
        "\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
        "\n",
        "        # Initialise neural network as a list using nn.Modulelist\n",
        "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
        "\n",
        "        self.iter = 0\n",
        "\n",
        "        # Xavier Normal Initialization\n",
        "        for i in range(len(layers)-1):\n",
        "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
        "            # set biases to zero\n",
        "            nn.init.zeros_(self.linears[i].bias.data)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "        if torch.is_tensor(x) != True:\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "    \n",
        "        #feature scaling\n",
        "\n",
        "        \n",
        "    def loss_BC(self,x,y):\n",
        "        \n",
        "    def loss_PDE(self, X_train_Nf):\n",
        "\n",
        "        g = X_train_Nf.clone()\n",
        "\n",
        "        g.requires_grad = True\n",
        "\n",
        "        u = self.forward(g)\n",
        "\n",
        "        u_x_t = autograd.grad(u,g,torch.ones([X_train_Nf.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
        "\n",
        "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(X_train_Nf.shape).to(device), create_graph=True)[0]\n",
        "\n",
        "        \n",
        "\n",
        "    def loss(self,x,y,X_train_Nf):\n",
        "\n",
        "        loss_u = self.loss_BC(x,y)\n",
        "        loss_f = self.loss_PDE(X_train_Nf)\n",
        "\n",
        "        loss_val = loss_u + loss_f\n",
        "\n",
        "        return loss_val\n",
        "\n",
        "\n",
        "    def closure(self):\n",
        "        \"\"\"\n",
        "        This is often used with optimization algorithms in PyTorch that need re-evaluation of the model, \n",
        "        such as L-BFGS. It computes the loss, performs backpropagation, and optionally (every 100 iterations) \n",
        "        tests the PINN and prints the loss and the test error.\n",
        "        \"\"\"\n",
        "        optimizer.zero_grad()\n",
        "        loss = self.loss(X_train_Nu, U_train_Nu, X_train_Nf)\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "\n",
        "        if self.iter % 100 == 0:\n",
        "            error_vec, _ = PINN.test()\n",
        "            print(loss,error_vec)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # test neural network\n",
        "    def test(self):\n",
        "        u_pred = self.forward(X_test)\n",
        "        error_vec = torch.linalg.norm((u-u_pred),2)/torch.linalg.norm(u,2)        # Relative L2 Norm of the error (Vector)\n",
        "        u_pred = u_pred.cpu().detach().numpy()\n",
        "        u_pred = np.reshape(u_pred,(256,100),order='F')\n",
        "\n",
        "        return error_vec, u_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOe9yRvxjakj"
      },
      "source": [
        "## Generate data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "go0WGbdx7bXB",
        "outputId": "c2124755-cbf9-4f71-b65d-be9f30bc5e52"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import scipy.io\n",
        "import numpy as np\n",
        "\n",
        "url = \"https://github.com/kks32-courses/sciml/raw/main/lectures/06-burgers/Burgers.mat\"\n",
        "\n",
        "# Download the file\n",
        "response = urllib.request.urlopen(url)\n",
        "matdata = response.read()\n",
        "# Write to local file \n",
        "with open('burgers.mat', 'wb') as f:\n",
        "  f.write(matdata)\n",
        "\n",
        "# Load Burger's mat file into dict\n",
        "data = scipy.io.loadmat('burgers.mat')\n",
        "x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
        "t = data['t']                                   # 100 time points between 0 and 1 [100x1]\n",
        "usol = data['usol']                             # solution of 256x100 grid points\n",
        "\n",
        "X, T = np.meshgrid(x,t)                         # makes 2 arrays X and T such that u(X[i],T[j])=usol[i][j] are a tuple\n",
        "plot3D(torch.from_numpy(x),torch.from_numpy(t),torch.from_numpy(usol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRKvROqlK8_i",
        "outputId": "5e019e19-dfc9-4f52-bf72-2f38ce10cbd9"
      },
      "outputs": [],
      "source": [
        "print(x.shape,t.shape,usol.shape)\n",
        "print(X.shape,T.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U66GsxSutJcB"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ZN_Jc-gtR-q"
      },
      "outputs": [],
      "source": [
        "X_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_test[0]  # [-1. 0.]\n",
        "ub = X_test[-1] # [1.  0.99]\n",
        "u_true = usol.flatten('F')[:,None] #Fortran style (Column Major)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrx8VgLR2AYK",
        "outputId": "e356ac31-03cd-44e8-c0d5-9b3607ce3461"
      },
      "outputs": [],
      "source": [
        "print(lb, ub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQxdtgBtsqJj"
      },
      "source": [
        "## Training Data\n",
        "\n",
        "Latin Hypercube Sampling (LHS) is a statistical method designed for generating a near-random sample of parameter values from a multidimensional distribution. The main advantage of LHS over simple random sampling is that it ensures that the entire range of each parameter is adequately sampled. This makes it particularly useful for computer simulations when a multidimensional parameter space needs to be explored.\n",
        "\n",
        "Here's a basic outline of how Latin Hypercube Sampling works:\n",
        "\n",
        "1. **Divide Each Dimension**: For each parameter (or dimension) you want to sample, divide the range of the parameter into NN non-overlapping intervals, where NN is the number of samples you desire. These intervals should have an equal probability of being sampled.\n",
        "\n",
        "1. **Random Sampling Without Replacement**: For each dimension, randomly sample a value from each interval. This ensures that every interval has a sampled value. The key here is that once an interval is sampled, it cannot be sampled again (i.e., no replacement).\n",
        "\n",
        "1. **Randomize Across Dimensions**: Combine the sampled values from all dimensions to generate a sample point. This process ensures that the sample points are randomly distributed across the entire multidimensional space.\n",
        "\n",
        "1. **Repeat**: Repeat steps 2 and 3 until you've obtained the desired number of sample points.\n",
        "\n",
        "Some key properties and advantages of LHS:\n",
        "\n",
        "1. **Stratification**: By design, LHS guarantees that the entire range of each parameter is sampled. This stratification ensures better space-filling properties than simple random sampling.\n",
        "\n",
        "1. **Reduction of Clustering**: Since each interval is sampled once and only once, clustering of sample points is reduced, which can be a problem in simple random sampling.\n",
        "\n",
        "1. **Efficiency**: For many problems, LHS can be more efficient than Monte Carlo sampling, as fewer samples might be needed to achieve the same level of precision.\n",
        "\n",
        "1. **Flexibility**: While LHS ensures even coverage of the input space, the actual sampled values within each interval are random, which provides flexibility in capturing the underlying distribution.\n",
        "\n",
        "In many applications, especially in computational experiments where evaluating the model is expensive, Latin Hypercube Sampling provides an efficient and effective way to explore the parameter space. It's widely used in sensitivity analysis, uncertainty analysis, and optimization problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjFW76MDzRXP"
      },
      "outputs": [],
      "source": [
        "# Boundary Conditions\n",
        "\n",
        "# Initial Condition -1 =< x =<1 and t = 0\n",
        "left_X = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
        "left_U = usol[:,0][:,None]\n",
        "\n",
        "# Boundary Condition x = -1 and 0 =< t =<1\n",
        "bottom_X = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
        "bottom_U = usol[-1,:][:,None]\n",
        "\n",
        "# Boundary Condition x = 1 and 0 =< t =<1\n",
        "top_X = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
        "top_U = usol[0,:][:,None]\n",
        "\n",
        "X_train = np.vstack([left_X, bottom_X, top_X])\n",
        "U_train = np.vstack([left_U, bottom_U, top_U])\n",
        "\n",
        "# Choose random N_u points for training\n",
        "idx = np.random.choice(X_train.shape[0], N_u, replace=False)\n",
        "\n",
        "X_train_Nu = X_train[idx, :]     # choose indices from  set 'idx' (x,t)\n",
        "U_train_Nu = U_train[idx,:]      # choose corresponding u\n",
        "\n",
        "# Collocation Points\n",
        "# Latin Hypercube sampling for collocation points\n",
        "# N_f sets of tuples(x,t)\n",
        "X_train_Nf = lb + (ub-lb)*lhs(2,N_f)\n",
        "X_train_Nf = np.vstack((X_train_Nf, X_train_Nu)) # append training points to collocation points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySJY_09LzRXz",
        "outputId": "4fd71f8c-4373-4fd3-afee-cd14c330a82c"
      },
      "outputs": [],
      "source": [
        "print(\"Original shapes for X and U:\",X.shape,usol.shape)\n",
        "print(\"Boundary shapes for the edges:\",left_X.shape,bottom_X.shape,top_X.shape)\n",
        "print(\"Available training data:\",X_train.shape,U_train.shape)\n",
        "print(\"Final training data:\",X_train_Nu.shape,U_train_Nu.shape)\n",
        "print(\"Total collocation points:\",X_train_Nf.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I82eH55ElVSq"
      },
      "source": [
        "# Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "id": "8KZvgKcalOVV",
        "outputId": "4c780c55-cd54-4b47-c56f-acbe0496244f"
      },
      "outputs": [],
      "source": [
        "# Convert to tensor and send to GPU\n",
        "X_train_Nf = torch.from_numpy(X_train_Nf).float().to(device)\n",
        "X_train_Nu = torch.from_numpy(X_train_Nu).float().to(device)\n",
        "U_train_Nu = torch.from_numpy(U_train_Nu).float().to(device)\n",
        "X_test = torch.from_numpy(X_test).float().to(device)\n",
        "u = torch.from_numpy(u_true).float().to(device)\n",
        "f_hat = torch.zeros(X_train_Nf.shape[0],1).to(device)\n",
        "\n",
        "PINN = FCN(layers)\n",
        "\n",
        "PINN.to(device)\n",
        "\n",
        "# Neural Network Summary\n",
        "print(PINN)\n",
        "\n",
        "params = list(PINN.parameters())\n",
        "\n",
        "# L-BFGS Optimizer\n",
        "optimizer = torch.optim.LBFGS(PINN.parameters(), lr,\n",
        "                              max_iter = steps,\n",
        "                              max_eval = None,\n",
        "                              tolerance_grad = 1e-11,\n",
        "                              tolerance_change = 1e-11,\n",
        "                              history_size = 100,\n",
        "                              line_search_fn = 'strong_wolfe')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "optimizer.step(PINN.closure)\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.2f' % (elapsed))\n",
        "\n",
        "\n",
        "# Model Accuracy\n",
        "error_vec, u_pred = PINN.test()\n",
        "\n",
        "print('Test Error: %.5f'  % (error_vec))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "8yI5eSZP5zD6",
        "outputId": "0222ef4e-cc65-4e8e-dc30-95114a4416e3"
      },
      "outputs": [],
      "source": [
        "solutionplot(u_pred,X_train_Nu.cpu().detach().numpy(),U_train_Nu)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpRh89zM8Te8"
      },
      "source": [
        "### Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "otgA5_xXqb4S"
      },
      "outputs": [],
      "source": [
        "x1=X_test[:,0]\n",
        "t1=X_test[:,1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90-ygJjca2c_"
      },
      "outputs": [],
      "source": [
        "arr_x1=x1.reshape(shape=X.shape).transpose(1,0).detach().cpu()\n",
        "arr_T1=t1.reshape(shape=X.shape).transpose(1,0).detach().cpu()\n",
        "arr_y1=u_pred\n",
        "arr_y_test=usol"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "ituXQapsciqE",
        "outputId": "03299016-5f4e-4ffb-ed02-c6448e622602"
      },
      "outputs": [],
      "source": [
        "plot3D_Matrix(arr_x1,arr_T1,torch.from_numpy(arr_y1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "xarXcLDdeF7X",
        "outputId": "2f4925ba-5c61-40e5-c79a-336828633ca3"
      },
      "outputs": [],
      "source": [
        "plot3D(torch.from_numpy(x),torch.from_numpy(t),torch.from_numpy(usol))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxDfWDTM1ZEd"
      },
      "source": [
        "# Inverse Solution to Burger's Equation\n",
        "\n",
        "$$\\frac{\\partial u}{\\partial t}+ \\lambda_1u\\frac{\\partial u}{\\partial x}=\\lambda_2\\frac{\\partial^2 u}{\\partial x^2} $$\n",
        "\n",
        "$$x\\in[-1,1]$$\n",
        "$$t\\in[0,1]$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MunY5McR1cBd"
      },
      "source": [
        "# Inverse Problems\n",
        "\n",
        "**Forward Problem**: Model $\\rightarrow$ Data (Predict)\n",
        "\n",
        "**Inverse Problem**: Data $\\rightarrow$ Model (i.e., actually, we get the Model's parameters)\n",
        "\n",
        "\n",
        "A Neural Network is an example of an inverse problem:\n",
        "\n",
        "1.   We have an unknown model that follows:\n",
        "\n",
        "$$N(X)=W_n\\sigma_{n-1}(W_{n-1}\\sigma_{n-2}(...(W_2(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n$$\n",
        "\n",
        "2.   We use our \"training\" data to get $W_i$ and $b_i$; for $i=1,2,..\\#layers$.\n",
        "\n",
        "\n",
        "Specifically:\n",
        "\n",
        "Data $\\rightarrow$ Model i.e., estimate our model parameters (i,e., $W_i$ and $b_i$)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTUoj_l_1cyk"
      },
      "source": [
        " \n",
        "# PINNs for Inverse Problems or \"Data-driven Discovery of Nonlinear Partial Differential Equations\"\n",
        "\n",
        "(See: https://arxiv.org/abs/1711.10566)\n",
        "\n",
        "**Inverse Problem**: Data $\\rightarrow$ Model's parameters so:\n",
        "\n",
        "Data $\\rightarrow$ PINN $\\rightarrow$ Model Parameters (i.e., our PDE parameters)\n",
        "\n",
        "For parameterized and nonlinear partial differential equations of the general form (Raissi et al., 2017):\n",
        "\n",
        "$$u_t+\\mathscr{N}[u;\\lambda]=0$$\n",
        "\n",
        "where, $u(x,t)$ is the hidden solution and $\\mathscr{N}[;\\lambda]$ is a nonlinear operator parameterized by $\\lambda$.\n",
        "\n",
        "In short: We will use a PINN to get $\\lambda$.\n",
        "\n",
        "Let:\n",
        "\n",
        "$u_t=\\frac{\\partial u}{\\partial t}$\n",
        "\n",
        "$u_x=\\frac{\\partial u}{\\partial x}$\n",
        "\n",
        "$u_{xx}=\\frac{\\partial^2 u}{\\partial x^2}$\n",
        "\n",
        "$\\mathscr{N}[u;\\lambda]=\\lambda_1u u_x-\\lambda_2 u_{xx}$\n",
        "\n",
        "Our PDE is described as:\n",
        "\n",
        "$$u_t+\\lambda_1uu_x=\\lambda_2u_{xx}$$\n",
        "\n",
        "If we rearrange our PDE, we get:\n",
        "\n",
        "$$u_t+\\lambda_1uu_x-\\lambda_2 u_{xx}=0$$\n",
        "\n",
        "Or,\n",
        "\n",
        "$$u_t+\\mathscr{N}[u;\\lambda]=0$$\n",
        "\n",
        "So we can use a PINN to obtain $\\lambda$!\n",
        "\n",
        "In our case (from the reference solution) $\\lambda=[\\lambda_1,\\lambda_2]=[1,\\nu]=\\left[1,\\frac{1}{100\\pi}\\right]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4h_OeTIc1hRn"
      },
      "source": [
        "## Neural Network\n",
        "\n",
        "A Neural Network is a function:\n",
        "\n",
        "$$N(X)=W_n\\sigma_{n-1}(W_{n-1}\\sigma_{n-2}(...(W_2(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n$$\n",
        "\n",
        "> Note: We usually train our N by iteratively minimizing a loss function in the training dataset(known data) to get $W_i$ and $b_i$.\n",
        "\n",
        "## PINNs=Neural Network + PDE\n",
        "(See: https://www.sciencedirect.com/science/article/pii/S0021999118307125)\n",
        "\n",
        "We can use a neural network to approximate any function (Universal APproximation Theorem): (See:https://book.sciml.ai/notes/03/)\n",
        "$$N(x,t)\\approx u(x,t)$$\n",
        "\n",
        "Since N(x,t) is a function, we can obtain its derivatives: $\\frac{\\partial N}{\\partial t},\\frac{\\partial^2 N}{\\partial x^2}, etc.$.(Automatic Diferentiation)\n",
        "\n",
        "Assume:$$N(t,x)\\approx u(t,x)$$\n",
        "\n",
        "Then:\n",
        "\n",
        "$$N_t+\\lambda_1NN_x-\\lambda_2 N_{xx}\\approx u_t+uu_x-\\lambda u_{xx}=0$$\n",
        "\n",
        "And:\n",
        "\n",
        "$$N_t+\\lambda_1NN_x-\\lambda_2 N_{xx}\\approx 0$$\n",
        "\n",
        "\n",
        "We define this function as $f$:\n",
        "\n",
        "$$f(t,x)=N_t+\\lambda_1NN_x-\\lambda_2N_{xx}$$\n",
        "\n",
        "Remember our operator:\n",
        "\n",
        "$$f(t,x)=N_t+\\mathscr{N}[N,\\lambda]$$\n",
        "\n",
        "So:\n",
        "\n",
        "$$f(t,x)\\approx 0$$\n",
        "\n",
        "### PINNs' Loss function\n",
        "\n",
        "We evaluate $f$ in a certain number of points ($N_u$) inside our domain $(x,t)$. Then we iteratively minimize a loss function related to $f$:\n",
        "\n",
        "$$MSE_f=\\frac{1}{N_u}\\sum^{N_u}_{i=1}|f(t_u^i,x_u^i)|^2$$\n",
        "\n",
        "Usually, the training data set is a set of points from which we know the answer. In our case, points inside our domain (i.e., interior points). **Remember that this is an inverse problem, so we know the data.**\n",
        "\n",
        "Since we know the outcome, we select $N$ and compute the $MSE_u$** (compare it to the reference solution).\n",
        "\n",
        "$$MSE_{u}=\\frac{1}{N_u}\\sum^{N_u}_{i=1}|u(t_{u}^i,x_u^i)-N(t_{u}^i,x_u^i)|^2$$\n",
        "\n",
        "Please note that $\\{t_u^i,x_u^i\\}_{i=1}^N$ are the same in $MSE_f$ and $MSE_u$\n",
        "\n",
        "#### Total Loss:\n",
        "\n",
        "$$MSE=MSE_{u}+MSE_f$$\n",
        "\n",
        "NOTE: We minimize $MSE$ to obtain the $N$'s parameters (i.e, $W_i$ and $b_i$) and the ODE parameters (i.e., $\\lambda$)$\\rightarrow$ We will ask our neural network to find our $\\lambda$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb4sJZoP1jKc"
      },
      "source": [
        "## Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6dcOu4b1lqU"
      },
      "outputs": [],
      "source": [
        "#  Deep Neural Network\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self,layers):\n",
        "        super().__init__() #call __init__ from parent class\n",
        "        self.activation = nn.Tanh()\n",
        "\n",
        "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
        "\n",
        "        for i in range(len(layers)-1):\n",
        "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
        "            # set biases to zero\n",
        "            nn.init.zeros_(self.linears[i].bias.data)\n",
        "\n",
        "    \n",
        "    def forward(self,x):\n",
        "        if torch.is_tensor(x) != True:\n",
        "            x = torch.from_numpy(x)\n",
        "\n",
        "        u_b = torch.from_numpy(ub).float().to(device)\n",
        "        l_b = torch.from_numpy(lb).float().to(device)\n",
        "\n",
        "        #preprocessing input\n",
        "        x = (x - l_b)/(u_b - l_b) #feature scaling\n",
        "\n",
        "        #convert to float\n",
        "        a = x.float()\n",
        "\n",
        "        for i in range(len(layers)-2):\n",
        "\n",
        "            z = self.linears[i](a)\n",
        "\n",
        "            a = self.activation(z)\n",
        "\n",
        "        a = self.linears[-1](a)\n",
        "\n",
        "        return a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpIu_rL-1oqZ"
      },
      "source": [
        "## Initialize our parameters\n",
        "\n",
        "Our operator is:\n",
        "\n",
        "$$\\mathscr{N}[u;\\lambda]=\\lambda_1u u_x-\\lambda_2 u_{xx}$$\n",
        "\n",
        "We know that:\n",
        "\n",
        "$$\\lambda_1=1$$\n",
        "\n",
        "$$\\lambda_2=\\nu=\\frac{1}{100\\pi}=0.003183$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAcFHqEp1oLy",
        "outputId": "ce489e7a-f007-4fe5-ec4b-6e5d462bd5c4"
      },
      "outputs": [],
      "source": [
        "lambda1=2.0\n",
        "lambda2=0.2\n",
        "print(\"Te real 𝜆 = [\", 1.0,nu,\"]. Our initial guess will be 𝜆 _PINN= [\",lambda1,lambda2,\"]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "amnAdvdH1oIm"
      },
      "outputs": [],
      "source": [
        "#  PINN\n",
        "class FCN():\n",
        "    def __init__(self, layers):\n",
        "\n",
        "        \n",
        "\n",
        "    def loss_data(self,x,y):\n",
        "        loss_u = self.loss_function(self.dnn(x), y)\n",
        "        return loss_u\n",
        "\n",
        "    def loss_PDE(self, X_train_Nu):\n",
        "        lambda1=self.lambda1\n",
        "        lambda2=self.lambda2\n",
        "\n",
        "        g = X_train_Nu.clone()\n",
        "        g.requires_grad = True\n",
        "\n",
        "        u = self.dnn(g)\n",
        "        u_x_t = autograd.grad(u,g,torch.ones([X_train_Nu.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
        "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(X_train_Nu.shape).to(device), create_graph=True)[0]\n",
        "        u_x = u_x_t[:,[0]]\n",
        "        u_t = u_x_t[:,[1]]\n",
        "        u_xx = u_xx_tt[:,[0]]\n",
        "\n",
        "        f = u_t + (lambda1)*(self.dnn(g))*(u_x) - (lambda2)*u_xx\n",
        "        loss_f = self.loss_function(f,f_hat)\n",
        "\n",
        "        return loss_f\n",
        "\n",
        "    def loss(self,x,y):\n",
        "\n",
        "        loss_u = self.loss_data(x,y)\n",
        "        loss_f = self.loss_PDE(x)\n",
        "\n",
        "        loss_val = loss_u + loss_f\n",
        "        return loss_val\n",
        "\n",
        "    \n",
        "    def closure(self):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = self.loss(X_train_Nu, U_train_Nu)\n",
        "        loss.backward()\n",
        "\n",
        "        self.iter += 1\n",
        "        if self.iter % 100 == 0:\n",
        "            error_vec, _ = PINN.test()\n",
        "            print(\n",
        "                'Relative Error(Test): %.5f , 𝜆_real = [1.0,  %.5f], 𝜆_PINN = [%.5f,  %.5f]' %\n",
        "                (\n",
        "                    error_vec.cpu().detach().numpy(),\n",
        "                    nu,\n",
        "                    self.lambda1.item(),\n",
        "                    self.lambda2.item()\n",
        "                )\n",
        "            )\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # test neural network\n",
        "    def test(self):\n",
        "\n",
        "        u_pred = self.dnn(X_true)\n",
        "        error_vec = torch.linalg.norm((U_true-u_pred),2)/torch.linalg.norm(U_true,2)        # Relative L2 Norm of the error (Vector)\n",
        "        u_pred = u_pred.cpu().detach().numpy()\n",
        "        u_pred = np.reshape(u_pred,(x.shape[0],t.shape[0]),order='F')\n",
        "\n",
        "        return error_vec, u_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZyxheyx2LBC"
      },
      "source": [
        "## Visualize the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "l0gD1Lo72MYL",
        "outputId": "a59b5bab-adb9-4300-d4a8-78e94f25e257"
      },
      "outputs": [],
      "source": [
        "x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
        "t = data['t']                                   # 100 time points between 0 and 1 [100x1]\n",
        "usol = data['usol']                             # solution of 256x100 grid points\n",
        "\n",
        "X, T = np.meshgrid(x,t)                         # makes 2 arrays X and T such that u(X[i],T[j])=usol[i][j] are a tuple\n",
        "plot3D(torch.from_numpy(x),torch.from_numpy(t),torch.from_numpy(usol))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8OdUI4b2Zz8",
        "outputId": "e77bc480-fb34-4edb-9263-dfe4eb926d13"
      },
      "outputs": [],
      "source": [
        "print(x.shape,t.shape,usol.shape)\n",
        "print(X.shape,T.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyGJYhHl2zUR"
      },
      "source": [
        "## Prepare Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_aPR1ib20iY"
      },
      "outputs": [],
      "source": [
        "X_true = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "# Domain bounds\n",
        "lb = X_true[0]  # [-1. 0.]\n",
        "ub = X_true[-1] # [1.  0.99]\n",
        "U_true = usol.flatten('F')[:,None] # (Column Major)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9RGvzRI22EW",
        "outputId": "2e3e1b42-0556-45e5-fc97-c3aa1181a298"
      },
      "outputs": [],
      "source": [
        "print(lb,ub)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXhoazDT25bl"
      },
      "source": [
        "## Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsaxBm6k26d5",
        "outputId": "5084d29f-3e11-4bb2-defa-27e56f4daef1"
      },
      "outputs": [],
      "source": [
        "total_points=len(x)*len(t)\n",
        "\n",
        "# Obtain random points for interior\n",
        "id_f = np.random.choice(total_points, N_f, replace=False)# Randomly chosen points for Interior\n",
        "X_train_Nu = X_true[id_f]\n",
        "U_train_Nu= U_true[id_f]\n",
        "\n",
        "print(\"We have\",total_points,\"points. We will select\",X_train_Nu.shape[0],\"points to train our model.\")\n",
        "\n",
        "print(\"Original shapes for X and U:\",X.shape,usol.shape)\n",
        "print(\"Final training data:\",X_train_Nu.shape,U_train_Nu.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxmPvCdO3AlA"
      },
      "source": [
        "## Train Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3XT4bmj3AK2"
      },
      "outputs": [],
      "source": [
        "# 'Convert our arrays to tensors and send them to our GPU'\n",
        "X_train_Nu = torch.from_numpy(X_train_Nu).float().to(device)\n",
        "U_train_Nu = torch.from_numpy(U_train_Nu).float().to(device)\n",
        "X_true = torch.from_numpy(X_true).float().to(device)\n",
        "U_true = torch.from_numpy(U_true).float().to(device)\n",
        "f_hat = torch.zeros(X_train_Nu.shape[0],1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULgTGHdI3F8i"
      },
      "outputs": [],
      "source": [
        "layers = np.array([2,20,20,20,20,20,20,20,20,1]) #8 hidden layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lNCt4DE3HmP",
        "outputId": "5510e283-5633-4273-a5ff-49a4501f9a24"
      },
      "outputs": [],
      "source": [
        "PINN = FCN(layers)\n",
        "\n",
        "# Neural Network Summary\n",
        "print(PINN)\n",
        "\n",
        "params = list(PINN.dnn.parameters())\n",
        "\n",
        "## Optimization\n",
        "\n",
        "### L-BFGS Optimizer\n",
        "optimizer = torch.optim.LBFGS(params, lr,\n",
        "                              max_iter = steps,\n",
        "                              max_eval = None,\n",
        "                              tolerance_grad = 1e-11,\n",
        "                              tolerance_change = 1e-11,\n",
        "                              history_size = 100,\n",
        "                              line_search_fn = 'strong_wolfe')\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "optimizer.step(PINN.closure)\n",
        "\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print('Training time: %.2f' % (elapsed))\n",
        "\n",
        "\n",
        "# Model Accuracy\n",
        "error_vec, u_pred = PINN.test()\n",
        "\n",
        "print('Test Error: %.5f'  % (error_vec))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS1xodXp3QLB"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "DFsVNWra3S9Z",
        "outputId": "f45c4ec9-d246-4e87-bbff-780035171288"
      },
      "outputs": [],
      "source": [
        "solutionplot(u_pred,X_train_Nu.cpu().detach().numpy(),U_train_Nu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "TFFGNrB43Vp8",
        "outputId": "395b062e-5a26-4824-aeef-382d9a13410e"
      },
      "outputs": [],
      "source": [
        "x1=X_true[:,0]\n",
        "t1=X_true[:,1]\n",
        "arr_x1=x1.reshape(shape=X.shape).transpose(1,0).detach().cpu()\n",
        "arr_T1=t1.reshape(shape=X.shape).transpose(1,0).detach().cpu()\n",
        "plot3D_Matrix(arr_x1,arr_T1,torch.from_numpy(u_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 871
        },
        "id": "82v43BCz3bkQ",
        "outputId": "7fd77e82-2add-4c10-d04a-5d8721b61316"
      },
      "outputs": [],
      "source": [
        "plot3D(torch.from_numpy(x),torch.from_numpy(t),torch.from_numpy(usol))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_21EpoD1fZi"
      },
      "source": [
        "## References:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUFzJJ23ZR2O"
      },
      "source": [
        "[1] Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2017). Physics informed deep learning (part i): Data-driven solutions of nonlinear partial differential equations. arXiv preprint arXiv:1711.10561. http://arxiv.org/pdf/1711.10561v1\n",
        "\n",
        "[2] Lu, L., Meng, X., Mao, Z., & Karniadakis, G. E. (1907). DeepXDE: A deep learning library for solving differential equations,(2019). URL http://arxiv. org/abs/1907.04502. https://arxiv.org/abs/1907.04502\n",
        "\n",
        "[3] Rackauckas Chris, Introduction to Scientific Machine Learning through Physics-Informed Neural Networks. https://book.sciml.ai/notes/03/\n",
        "\n",
        "[4] Repository: Physics-Informed-Neural-Networks (PINNs).https://github.com/omniscientoctopus/Physics-Informed-Neural-Networks/tree/main/PyTorch/Burgers'%20Equation\n",
        "\n",
        "[5]  Raissi, M., Perdikaris, P., & Karniadakis, G. E. (2017). Physics Informed Deep Learning (part ii): Data-driven Discovery of Nonlinear Partial Differential Equations. arXiv preprint arXiv:1711.10566. https://arxiv.org/abs/1711.10566\n",
        "\n",
        "[6] Repository: PPhysics-Informed Deep Learning and its Application in Computational Solid and Fluid Mechanics.https://github.com/alexpapados/Physics-Informed-Deep-Learning-Solid-and-Fluid-Mechanics.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "Y61YA90-WIZ1",
        "i0j73GoH86-m"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
