{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "# 08: DeepONet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Exercise:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/08-deeponet/08-deeponet-exercise.ipynb)\n",
        "**Solution:** [![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kks32-courses/sciml/blob/main/lectures/08-deeponet/08-deeponet.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problems with PINNs\n",
        "\n",
        "We represent a multi-layer perceptron (MLP) $y = \\mathbf{\\mathcal{F}}(x;\\theta)$ as a function that takes an input $x \\in Ro^d$ and gives an output $y \\in Ro^d$ with trainable weights $\\theta$. A Physics Informed Neural Network (PINN) can be described as  $\\mathbf{u}(x) = \\mathbf{\\mathcal{F}}(x;\\theta)$ taking as input the independent variable $x$ of the underlying Partial Differential Equation (PDE) and outputs its solution, $\\mathbf{u}(x)$.  The training process for a PINN involves minimizing the combined residual of the PDE and its boundary conditions.\n",
        "\n",
        "To illustrate, consider the PDE\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla \\cdot (\\kappa \\nabla u) &= f(x), \\quad x \\in \\Omega = [0,1] \\times [0,1]\\\\\n",
        "  u(x) & = g(x), \\quad x \\in \\partial \\Omega\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "and train a PINN $\\mathcal{F}(x;\\theta)$ to minimize the loss function\n",
        "$$\n",
        "\\Pi(\\theta) = \\frac{1}{N_v} \\sum_{i=1}^{N_v} | \\nabla \\cdot (\\kappa \\nabla \\mathcal{F}(x_i;\\theta)) - f(x_i) |^2 + \\frac{\\lambda_b}{N_b} \\sum_{i=1}^{N_b} | \\mathcal{F}(x_i;\\theta) - g(x_i) |^2\n",
        "$$\n",
        "\n",
        "The optimal weights are given by $\\theta^* = \\argmin{\\theta} \\Pi(\\theta)$  and the corresponding PINN solution for the above PDE becomes $u(x) =\\mathcal{F}(x;\\theta^*)$. \n",
        "\n",
        "However, if we alter either $f$ or $g$ in the PDE, the previously trained network may no longer be applicable. Essentially, we would need to retrain the network (potentially with the same structure) for the updated $f$ and $g$ values. This repeated training can be inefficient and is something we'd prefer to bypass. In the subsequent sections, we'll explore strategies to address this challenge.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameterized PDE\n",
        "Consider the source term $f$ in the above PDE is given as a parametric function $f(x;\\alpha)$. For instance, we could have\n",
        "$$\n",
        "f(x_1,x_2;\\alpha) = 4 \\alpha x_1 (1-x_1) (1-x_2) \n",
        "$$\n",
        "Then we could train a PINN that accommodates for the parametrization by considering a network that takes as input both $x$ and $\\alpha$, i.e., $\\mathcal{F}(x, \\alpha;\\theta)$.\n",
        "\n",
        "This network can be trained by minimizing the loss function\n",
        "$$\n",
        "\\Pi(\\theta) = \\frac{1}{N_a} \\sum_{j=1}^{N_a}\\left[\\frac{1}{N_v} \\sum_{i=1}^{N_v} | \\nabla \\cdot (\\kappa \\nabla \\mathcal{F}(x_i,\\alpha_j;\\theta)) - f(x_i,\\alpha_j) |^2 + \\frac{\\lambda_b}{N_b} \\sum_{i=1}^{N_b} | \\mathcal{F}(x_i,\\alpha_j;\\theta) - g(x_i) |^2 \\right]\n",
        "$$\n",
        "\n",
        "We have to also consider collocation points for the parameter $\\alpha$ while constructing the loss function. If $\\theta^* = \\argmin{\\theta} \\Pi(\\theta)$, then the solution to the parameterized PDE would be $u(x,\\alpha) = \\mathcal{F}(x, \\alpha;\\theta^*) $. Further, for any new value of $\\alpha = \\hat{\\alpha}$ we could find the solution by evaluating $\\mathcal{F}(x,\\hat{\\alpha};\\theta^*)$. We could use the same approach if there was a way of parameterizing the functions $\\kappa(x)$ and $g(x)$. \n",
        "\n",
        "![Parameterized PINNs](parameterized-pinn.png)\n",
        "> Schematic of a PINN with a parameterized input\n",
        "\n",
        "\n",
        "However, what if we wanted the solution for an arbitrary, non-parametric $f$?  In order to do this, we need to find a way to approximate **operators that map functions to functions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions and Operators:\n",
        "\n",
        "### Function:\n",
        "\n",
        " Maps between vector spaces:\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Let $f_1(x)=sin(x)$; for  $x\\in\\mathbf{R}$\n",
        "\n",
        "$$z=f_1(x)=sin(x)\\in[0:1]$$\n",
        "\n",
        "In other words $f_1$ maps $\\mathbf{R}→[0,1]$\n",
        "\n",
        "### Operator:\n",
        "\n",
        " Maps between infite-dimensional function spaces:\n",
        "$$G(f_1(x))=f_2(x)$$\n",
        "\n",
        "*Example:*\n",
        "\n",
        "Derivative Operator →$\\frac{d}{d x}$\n",
        "\n",
        "It transforms a funcion $f_1$ into a function $f_2$:\n",
        "\n",
        "Let $f_1(x)=sin(x)$\n",
        "\n",
        "Then when we apply our operator:\n",
        "\n",
        "$$f_2=\\frac{df_1(x)}{d x}=\\frac{d}{d x}sin(x)=cos(x)$$\n",
        "\n",
        "\n",
        "### Parametric PDEs and Operators\n",
        "\n",
        "Parametric PDEs $\\rightarrow$ Some parameters (e.g., shape, IC/BC, coefficients, etc.) of a given PDE system are allowed to change.\n",
        "\n",
        "Let  $\\mathcal{N}$ be a nonlinear operator. Let's consider a parametric PDEs of the form:\n",
        "\n",
        "$$\\mathcal{N}(u,s)=0$$\n",
        "\n",
        "Where $u$ is the input function and $s$ is the unknown PDE's solution (also a function).\n",
        "\n",
        "Our PDE solution operator would be:\n",
        "\n",
        "$$G(u)=s$$\n",
        "\n",
        "**Note 1:** In other words, we can express the general solution of our PDE as an operator $G$\n",
        "\n",
        "**Note 2:** Remember $s$ is itself a function, so if we evaluate it at any point $y$, the answer would be a real number:\n",
        "\n",
        " $$G(u)(y)=s(y)\\in \\mathbf{R}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The universal approximation theorem states that neural networks can be used to approximate any continuous function to arbitrary accuracy if no constraint is placed on the width and depth of the hidden layers. However, another approximation result, which is yet more surprising and has not been appreciated so far, states that a neural network with \n",
        "a single hidden layer can approximate accurately any nonlinear continuous __functional__ (a mapping from a space of functions into the real numbers) or (nonlinear) operator (a mapping from a space of functions into another space of functions).\n",
        "\n",
        "Before reviewing the approximation theorem for operators, we introduce some notation, which will be used through this paper. Let $G$ be an operator taking an input function $u$, and then $G(u)$ is the corresponding output function. For any point $y$ in the domain of $G(u)$,  the output $G(u)(y)$ is a real number. Hence, the network takes inputs composed of two parts: $u$ and $y$, and outputs $G(u)(y)$. Although our goal is to learn operators, which take a function as the input, we have to represent the input functions discretely, so that network approximations can be applied. A straightforward and simple way, in practice, is to employ the function values at sufficient but finite many locations $\\{x_1, x_2, \\dots, x_m\\}$; we call these locations as ``sensors''.\n",
        "\n",
        "\n",
        "## Universal Approximation Theorem for Operator\n",
        "\n",
        "Suppose that $\\sigma$ is a continuous non-polynomial function, $X$ is a Banach Space, $K_1 \\subset X$, $K_2 \\subset \\mathbb{R}^d$ are two compact sets in $X$ and $\\mathbb{R}^d$, respectively, $V$ is a compact set in $C(K_1)$, $G$ is a nonlinear continuous operator, which maps $V$ into $C(K_2)$. Then for any $\\epsilon>0$, there are positive integers $n$, $p$, $m$, constants $c_i^k, \\xi_{ij}^k, \\theta_i^k, \\zeta_k \\in \\mathbb{R}$, $w_k \\in \\mathbb{R}^d$, $x_j \\in K_1$, $i=1,\\dots,n$, $k=1,\\dots,p$, $j=1,\\dots,m$, such that\n",
        "$$\n",
        "\\left|G(u)(y) - \\sum_{k=1}^p\n",
        "\\underbrace{\\sum_{i=1}^n c_i^k \\sigma\\left(\\sum_{j=1}^m \\xi_{ij}^ku(x_j)+\\theta_i^k\\right)}_{branch}\n",
        "\\underbrace{\\sigma(w_k \\cdot y+\\zeta_k)}_{trunk}\n",
        "\\right|<\\epsilon  \n",
        "$$\n",
        "holds for all $u \\in V$ and $y \\in K_2$.\n",
        "\n",
        "![DeepONet](sensors-deeponet.png)\n",
        "> Illustrations of the problem setup and architectures of DeepONets.} (\\textbf{A}) The network to learn an operator $G: u \\mapsto G(u)$ takes two inputs $[u(x_1), u(x_2), \\dots, u(x_m)]$ and $y$. (\\textbf{B}) Illustration of the training data. For each input function $u$, we require that we have the same number of evaluations at the same scattered sensors $x_1, x_2, \\dots, x_m$. However, we do not enforce any constraints on the number or locations for the evaluation of output functions. (\\textbf{C}) The stacked DeepONet in Theorem~\\ref{thm:main} has one trunk network and $p$ stacked branch networks. (\\textbf{D}) The unstacked DeepONet has one trunk network and one branch network.\n",
        "\n",
        "The universal approximation theorem suggests that neural networks have the potential to learn nonlinear operators from data, much like current deep learning techniques. However, while the theorem indicates that fully-connected neural networks (FNNs) can theoretically approximate functions, they often underperform when compared to specialized architectures like convolutional neural networks (CNNs) in practical scenarios. This is primarily because the theorem only addresses approximation errors and neglects the equally critical optimization and generalization errors. For a neural network to be effective, it must not only approximate well but also train efficiently and generalize to new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DeepONet\n",
        "\n",
        "We focus on learning operators in a more general setting, where the only requirement for the training dataset is the consistency of the sensors $\\{x_1, x_2, \\dots, x_m\\}$ for input functions. In this general setting, the network inputs consist of two separate components:  $[u(x_1), u(x_2), \\dots, u(x_m)]^T$ and $y$ (see above Fig A), and the goal is to achieve good performance by designing the network architecture. One straightforward solution is to directly employ a classical network, such as FNN, CNN or RNN, and concatenate two inputs together as the network input, i.e., $[u(x_1), u(x_2), \\dots, u(x_m), y]^T$. However, the input does not have any specific structure, and thus it is not meaningful to choose networks like CNN or RNN. Here we use FNN as the baseline model.\n",
        "\n",
        "In high dimensional problems, $y$ is a vector with $d$ components, so the dimension of $y$ does not match the dimension of $u(x_i)$ for $i=1,2,\\dots,m$ any more. This also prevents us from treating $u(x_i)$ and $y$ equally, and thus at least two sub-networks are needed to handle $[u(x_1), u(x_2), \\dots, u(x_m)]^T$ and $y$ separately. Although the universal approximation theorem does not have any guarantee on the total error, it still provides us a network structure. The theorem only considers a shallow network with one hidden layer, so we extend it to deep networks, which have more expressivity than shallow ones. The architecture we propose is shown in Fig. C, and the details are as follows. First there is a \"trunk\" network, which takes $y$ as the input and outputs $[t_1, t_2, \\dots, t_p]^T \\in \\mathbb{R}^p$. In addition to the trunk network, there are $p$ \"branch\" networks, and each of them takes $[u(x_1), u(x_2), \\dots, u(x_m)]^T$ as the input and outputs a scalar $b_k \\in \\mathbb{R}$ for $k=1,2,\\dots,p$. We merge them together as:\n",
        "\n",
        "$$ G(u)(y) \\approx \\sum_{k=1}^p b_k t_k. $$\n",
        "\n",
        "We note that the trunk network also applies activation functions in the last layer, i.e., $t_k = \\sigma(\\cdot)$ for $k=1,2,\\dots,p$, and thus this trunk-branch network can also be seen as a trunk network with each weight in the last layer parameterized by another branch network instead of the classical single variable. We also note that in Eq. \\ref{eq:thm} the last layer of each $b_k$ branch network does not have bias. Although bias is not necessary in Theorem \\ref{thm:main}, adding bias may increase the performance by reducing the generalization error. In addition to adding bias to the branch networks, we may also add a bias $b_0 \\in \\mathbb{R}$ in the last stage:\n",
        "\n",
        "$$\n",
        "G(u)(y) \\approx \\sum_{k=1}^p b_k t_k + b_0.\n",
        "$$\n",
        "\n",
        "In practice, $p$ is at least of the order of 10, and using lots of branch networks is computationally and memory expensive. Hence, we merge all the branch networks into one single branch network (Fig. D), i.e., a single branch network outputs a vector $[b_1, b_2, \\dots, b_p]^T \\in \\mathbb{R}^p$. When $p$ branch networks stacked parallel it is `stacked DeepONet`, while the other is termed `unstacked DeepONet`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A standard DeepONet comprises two neural networks. We describe below its construction to approximate an operator $\\mathcal{N} : A \\rightarrow U$, where $A$ is a set of functions of the form $a: \\Omega_X \\subset Ro^d \\rightarrow Ro$ while $U$ consists of functions of the form $u: \\Omega_Y \\subset Ro^D \\rightarrow Ro$. Furthermore, we assume that point-wise evaluations of both class of functions is possible. The architecture for the DeepONet is explained below:\n",
        "\n",
        "\n",
        "* Fix $M$ distinct sensor points $x^{(1)},..., x^{(M)}$ in $\\Omega_X$.\n",
        "* Sample a function $a \\in A$ at these sensor points to get the vector $\\mathbf{a} =  [a(x^{(1)}), ... , \\ a(x^{(M)})]^\\top \\in Ro^M$.\n",
        "\n",
        "* Supply $\\mathbf{a}$ as the input to a sub-network, called the _branch net_ $\\mathcal{B}(.;\\mathbf{\\theta}_B):Ro^M \\rightarrow Ro^p$, whose output would be the vector $\\mathbf{\\beta} = [\\beta_1(\\mathbf{a}), ..., \\ \\beta_p(\\mathbf{a})]^\\top\\in Ro^p$. Here $\\mathbf{\\theta}_B$ are the trainable parameters of the branch net. The dimension of the output of the branch is relatively small, say $p \\approx 100$.\n",
        "\n",
        "* Supply $x$ as an input to a second sub-network, called the _trunk net_ $\\mathcal{T}(.;\\mathbf{\\theta}_T): Ro^D \\rightarrow Ro^p$, whose output would be the vector $\\mathbf{\\tau} = [\\tau_1(u), ..., \\ \\tau_p(u)]^\\top \\in Ro^p$. Here $\\mathbf{\\theta}_T$ are the trainable parameters of the trunk net.\n",
        "\n",
        "* Take a dot product of the outputs of the branch and trunk nets to get the final output of the DeepONet $\\widetilde{\\mathcal{N}}(.,.;\\mathbf{\\theta}):Ro^D \\times Ro^M \\rightarrow Ro$ which will approximate the value of $u(y)$\n",
        "\n",
        "\n",
        "$u(y) \\approx \\widetilde{\\mathcal{N}}(y,\\mathbf{a};\\mathbf{\\theta}) = \\sum_{k=1}^p \\beta_k(\\mathbf{a}) \\tau_k(y).$\n",
        "\n",
        "where the trainable parameters of the DeepONet will be the combined parameters of the branch and trunk nets, i.e., $\\mathbf{\\theta} = [\\theta_T, \\theta_M]$.\n",
        "\n",
        "![DeepONet](DeepONet.png)\n",
        "> Schematic of a DeepONet\n",
        "\n",
        "In the above construction, once the DeepONet is trained (we will discuss the training in the following section), it will approximate the underlying operator $\\mathcal{N}$, and allow us to approximate the value of any $\\mathcal{N}(a)(y)$ for any $a \\in A$ and any $x \\in \\Omega_Y$. Note that in the construction of the DeepONet, the $M$ sensor points need to be pre-defined and cannot change during the training and evaluation phases.\n",
        "\n",
        "\n",
        "We can make the following observations regarding the DeepONet architecture:\n",
        "\n",
        "* The expression in (\\ref{eq:deeponet}) has the form of representing the solution as the sum of a series of coefficients and functions. The coefficients are determined by the branch network, while the functions are determined by the trunk network. In that sense the DeepONet construction is similar to that of what is used in the spectral method or the finite element method. There is a critical difference though. In these methods, the basis functions are pre-determined and selected by the user. However, in the DeepONet these functions are determined by the trunk network and their final form depends on the data used to train the DeepONet. \n",
        "\n",
        "* Architecture of the branch sub-network: When points for sampling the input function are chosen randomly, the appropriate architecture for the branch network comprises fully connected layers. Further recognizing that the dimension of the input to this network can be rather large $N_1 \\approx 10^4$, while the output is typically small $p \\approx 10^2$, this network can be thought of as an encoder. \n",
        "    \n",
        "* When points for sampling the input function are chosen on a uniform grid, the appropriate architecture for the branch network comprises convolutional layer layers. In that case this network maps an image of large dimension ($N_1 \\approx 10^4$) to a latent vector of small dimension, $p \\approx 10^2$. Thus it is best represented by a convolutional neural network.  \n",
        "    \n",
        "* Broadly speaking, there are two ways of improving the experssivity of the DeepONet. These involve increase the number of network parameters in the branch and trunk sub-networks, and increasing the dimension $p$ of the latent vectors formed by these sub-networks. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training DeepONets\n",
        "Training a DeepONet is typically supervised, and requires pairwise data. The following are the main steps involved:\n",
        "\n",
        "* Select $N_1$ representative function $a^{(i)}$, $1 \\leq i \\leq N_1$ from the set $A$. Evaluate the values of these functions at the $M$ sensor points, i.e., $a^{(i)}_j = a^{(i)}(x^{(j)})$ for $1 \\leq j \\leq M$. This gives us the vectors $\\mathbf{a}^{(i)} = [a^{(i)}(x^{(1)}),...,a^{(i)}(x^{(M)})]^\\top \\in Ro^M$ for each $1 \\leq i \\leq N_1$.\n",
        "\n",
        "* For each $a^{(i)}$, determine (numerically or analytically) the corresponding functions $u^{(i)}$ given by the operator $\\mathcal{N}$. \n",
        "\n",
        "* Sample the function $u^{(i)}$ at $N_2$ points in $\\Omega_Y$, i.e., $u^{(i)}(y^{(k)})$ for $1 \\leq k \\leq N_2$. \n",
        "\n",
        "* Construct the training set \n",
        "\n",
        "$$\n",
        "\\mathcal{S} = \\left \\{\\Big(\\mathbf{a}^{(i)}, y^{(k)}, u^{(i)}(y^{(k)})\\Big) : 1 \\leq i \\leq N_1, \\ 1 \\leq k \\leq N_2 \\right\\}\n",
        "$$\n",
        "\n",
        "which will have $N_1 \\times N_2$ samples.\n",
        "\n",
        "* Define the loss function\n",
        "\n",
        "$$\n",
        "\\Pi(\\theta) = \\frac{1}{N_1 N_2} \\sum_{i=1}^{N_1} \\sum_{k=1}^{N_2} | \\widetilde{N}(y^{(k)}, \\mathbf{a}^{(i)};\\theta) - u^{(i)}(y^{(k)})|^2.\n",
        "$$\n",
        "\n",
        "* Training the DeepONet corresponds to finding $\\theta^* = \\argmin{\\theta} \\Pi(\\theta)$.\n",
        "\n",
        "* Once trained, then given any new $a \\in A$ samples at the $M$ sensor points (which gives the vector $\\mathbf{a} \\in Ro^M$), and a new point $y \\in \\Omega_Y$, we can evaluate the corresponding prediction $u^*(y) = \\widetilde{N}(y, \\mathbf{a};\\theta^*)$. \n",
        "\n",
        "> Note:\n",
        "We need not choose the same $N_2$ points across all $i$ in the training set. In fact, these can be chosen randomly leading to a more diverse dataset.\n",
        "\n",
        "> Note:\n",
        "The DeepONet can be easily extended to the case where the input comprises multiple functions. In this case, the trunk network remains the same, however the branch network now has multiple vectors as input. The case corresponding to two input functions, $a(x)$ and $b(x)$, which when sampled yield the vectors, $\\mathbf{a}$ and $\\mathbf{b}$.\n",
        "\n",
        "> Note:\n",
        "The DeepONet can be easily extended to the case where the output comprises multiple functions (say $D$ such functions). In this case, the output of the branch and trunk network leads to $D$ vectors each with dimension $p$. The solution is then obtained by taking the dot product of each one of these vectors.  The case corresponding to two output functions $u_1(y)$ and $u_2(y)$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhPIO4gwxTcb"
      },
      "source": [
        " ### Universal Approximation Theorem for Operator\n",
        "\n",
        " $\\forall \\epsilon >0$, there are positive integers $n,p,m$, constants $c_i^k,W_{bij}^k,b_{bij}^k,W_{tk},b_{tk}$ such that:\n",
        "\n",
        "$$\\left|G(u)(y)-\\sum_{k=1}^{p}\\sum_{i=1}^{n}c_i^k\\sigma\\left(\\sum_{j=1}^{m}W_{bij}^{k}u(x_j)+b_{bi}^k\\right).\\sigma(W_{tk}.y+b_{tk})\\right|<\\epsilon $$\n",
        "\n",
        " ### Neural Network\n",
        "\n",
        "A Neural Network is a function that takes the form:\n",
        "\n",
        "$$NN(X)=W_n\\sigma_{n-1}(W_{n-1}\\sigma_{n-2}(...(W_2\\sigma_1(W_1X+b_1)+b_2)+..)+b_{n-1})+b_n$$\n",
        "\n",
        "So we can use 2 NNs to implement the Universal Approximation Theorem for Operator i.e.:\n",
        "\n",
        "Branch:\n",
        "\n",
        "$$NN_b(u(\\textbf{x}))=b(u(\\textbf{x}))=\\textbf{c}.\\sigma\\left(W_{b}u(\\textbf{x})+\\textbf{b}_{b}\\right)$$\n",
        "\n",
        "Trunk:\n",
        "\n",
        "\n",
        "$$NN_t(\\textbf{y})=t(\\textbf{y})=\\sigma(W_{t}.\\textbf{y}+\\textbf{b}_{t})$$\n",
        "\n",
        "### DeepOnet\n",
        "\n",
        "Learn the solution operators of parametric PDEs → We will try to approximate $G$  (the solution of our PDE operator) by two neural networks:\n",
        "\n",
        "\n",
        "$$G_\\theta(u)(y)=\\sum_{k=1}^q\\underset{Branch}{\\underbrace{b_k\\left(u(x_1),u(x_2),...,u(x_m)\\right)}}.\\underset{Trunk}{\\underbrace{t_k(\\textbf{y})}}$$\n",
        "\n",
        "\n",
        "We want to obtain G, so our goal would be:\n",
        "\n",
        " $$G_\\theta(u)(y)\\approx G(u)(y)$$\n",
        "\n",
        "So we will enforce that condition into a loss function:\n",
        "\n",
        "$$\\mathcal{L}(\\theta)=\\frac{1}{NP}\\sum_{i=1}^N\\sum_{j=1}^P\\left|G_{\\theta}(u^{(i)})y_j^{(i)}-G(u^{(i)})y_j^{(i)}\\right|^2$$\n",
        "\n",
        "\n",
        "$$\\mathcal{L}(\\theta)=\\frac{1}{NP}\\sum_{i=1}^N\\sum_{j=1}^P\\left|\\sum_{k=1}^q{b_k\\left(u(x_1),u(x_2),...,u(x_m)\\right)}.t_k(y_j^{(i)})-G(u^{(i)})y_j^{(i)}\\right|^2$$\n",
        "\n",
        "where $N$ is the number of functions $u(x)$ in our training dataset, $P$, is the number of points inside the domain at which we will evaluate $G(u)$. \n",
        "\n",
        "$m:$ Number of points at which we evaluated our input functions.\n",
        "\n",
        "$N:$ Number of input functions.\n",
        "\n",
        "$P:$ Number of points at which we evaluate the output function $\\rightarrow$ output sensors.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-o-HC2x4T9L"
      },
      "source": [
        "**In summary:**\n",
        "\n",
        "To train a DeepOnet, we would:\n",
        "\n",
        "1.   We select $N$ functions →$u(x)$.\n",
        "2.   We evaluate our $N$ functions at $m$ points (i.e., input sensors) →$u(x_1),u(x_2),...,u(x_m)$\n",
        "3.   We send the $m$ outputs of our $N$ functions to our **branch network** → $b_k(u(x_1),u(x_2),...,u(x_m))$\n",
        "4.   We select $P$ points (i.e., output sensors) inside our domain → $y_1,y_2,...,y_P$\n",
        "5.  We send our output sensors to our **trunk network**→$t_k(y_1,y_2,...,y_P)$\n",
        "6. We approximate our operator by computing the dot product between the outpur of our **branch network** and the output of our **trunk network**→ $G_\\theta(u)(y)=\\sum_{k=1}^q\\underset{Branch}{\\underbrace{b_k\\left(u(x_1),u(x_2),...,u(x_m)\\right)}}.\\underset{Trunk}{\\underbrace{t_k(\\textbf{y})}}$\n",
        "7. Ideally $G_\\theta(u)(y)\\approx G(u)(y)$, so we compute the error → $\\mathcal{L}(\\theta)=\\frac{1}{NP}\\sum_{i=1}^N\\sum_{j=1}^P\\left|G_{\\theta}(u^{(i)})y_j^{(i)}-G(u^{(i)})y_j^{(i)}\\right|^2$\n",
        "\n",
        "8. We update our NN parameters (i.e., branch and trunk) to minimize  $\\mathcal{L}(\\theta)$.\n",
        "9. We repeat the process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-qnbwQ5DTuw"
      },
      "source": [
        "# Problem Setup\n",
        "\n",
        "## Anti-derivative Operator\n",
        "\n",
        "Our PDE would be:\n",
        "\n",
        "$$\\frac{ds(x)}{dx}-u(x)=0$$\n",
        "\n",
        "\n",
        "The solution of our PDE is:\n",
        "\n",
        "$$G:u(x)→s(x)=s(0)+\\int_{0}^{x}u(t)dt$$\n",
        "\n",
        "$$x\\in[0,1]$$\n",
        "$$s(0)=0$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as onp\n",
        "import jax.numpy as np\n",
        "from jax import random, grad, vmap, jit\n",
        "from jax.experimental import optimizers\n",
        "from jax.experimental.ode import odeint\n",
        "from jax.nn import relu\n",
        "from jax.config import config\n",
        "\n",
        "import itertools\n",
        "from functools import partial\n",
        "from torch.utils import data\n",
        "from tqdm import trange\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvY9PrxfylwH"
      },
      "source": [
        "# Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phC4C1GNDrvA"
      },
      "source": [
        "We will randomly sample 10000 different functions $u$ from a zero-mean Gaussian process with an exponential quadratic kernel with a length scale: $l=0.2$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-lQbo2kYcvA"
      },
      "outputs": [],
      "source": [
        "N_train = 10000\n",
        "m = 100 # number of input sensors\n",
        "P_train = 1   # number of output sensors\n",
        "length_scale = 0.2 #lenght_scale for the exponential quadratic kernel\n",
        "key_train = random.PRNGKey(0)  # use different key for generating training data and test data \n",
        "config.update(\"jax_enable_x64\", True) # Enable double precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gupNZ3GKDSDu"
      },
      "source": [
        "## Generate a random function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RBF (Radial Basis Function) Kernel\n",
        "\n",
        "An RBF kernel, often termed the Gaussian kernel, is a widely-used kernel function in the realms of machine learning and statistics. It finds predominant applications in support vector machines (SVMs) and kernel principal component analysis (PCA).\n",
        "\n",
        "The mathematical expression for the RBF kernel is:\n",
        "\n",
        "$$K(\\mathbf{x}, \\mathbf{y}) = \\exp\\left(-\\frac{||\\mathbf{x} - \\mathbf{y}||^2}{2\\sigma^2}\\right)$$\n",
        "\n",
        "Where:\n",
        "- $K(\\mathbf{x}, \\mathbf{y})$ represents the kernel value between data points $\\mathbf{x}$ and $\\mathbf{y}$.\n",
        "- $||\\mathbf{x} - \\mathbf{y}||$ denotes the Euclidean distance separating the two data points.\n",
        "- $\\sigma$ is a freely adjustable parameter, often referred to as the bandwidth or spread of the kernel. The magnitude of $\\sigma$ dictates the reach or influence of individual data points when assessing similarity. A diminutive $\\sigma$ yields a constricted kernel, implying each point has a restricted zone of influence. Conversely, an enlarged $\\sigma$ produces a more expansive kernel, with data points influencing a broader region.\n",
        "\n",
        "The intrinsic concept behind the RBF kernel is to gauge the similarity between two points in the input space. The function's value peaks (i.e., 1) when both points overlap and diminishes (approaching 0) as the points diverge. The decrement rate is modulated by the $\\sigma$ parameter.\n",
        "\n",
        "The term \"radial basis\" originates from the kernel's value being contingent solely on the distance (i.e., radially symmetric) between the two points, rather than their absolute coordinates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define RBF kernel\n",
        "def RBF(x1, x2, params):\n",
        "    output_scale, lengthscales = params\n",
        "    diffs = np.expand_dims(x1 / lengthscales, 1) - \\\n",
        "            np.expand_dims(x2 / lengthscales, 0)\n",
        "    r2 = np.sum(diffs**2, axis=2)\n",
        "    return output_scale * np.exp(-0.5 * r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_us(x,u,y,s):\n",
        "  fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "  plt.rcParams['font.size'] = '18'\n",
        "  color='black'\n",
        "  wdt=1.5\n",
        "  ax1.plot(x,u,'k--',label='$u(x)=ds/dx$',linewidth=wdt)\n",
        "  ax1.plot(y,s,'o-',label='$s(x)=s(0)+\\int u(t)dt|_{t=y}$',linewidth=wdt)\n",
        "  ax1.set_xlabel('x',fontsize='large')\n",
        "  ax1.set_ylabel('u',fontsize='large')\n",
        "  ax1.tick_params(axis='y', color=color)\n",
        "  ax1.legend(loc = 'lower right', ncol=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ol-umrOeGD0E"
      },
      "outputs": [],
      "source": [
        "# Sample GP prior at a fine grid\n",
        "N = 512\n",
        "gp_params = (1.0, length_scale)\n",
        "jitter = 1e-10\n",
        "X = np.linspace(0, 1, N)[:,None]\n",
        "K = RBF(X, X, gp_params)\n",
        "L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "gp_sample = np.dot(L, random.normal(key_train, (N,)))\n",
        "\n",
        "# Create a callable interpolation function  \n",
        "u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "# Input sensor locations and measurements\n",
        "x = np.linspace(0, 1, m)\n",
        "u = vmap(u_fn, in_axes=(None,0))(0.0, x) #vectorize our code to run it in multiple batches simultaneusly (or to evaluate a function simultaneusly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHzsz9XlEzCy"
      },
      "source": [
        "We obtain the corresponding 10000 ODE solutions by solving: \n",
        "\n",
        "$$\\frac{ds(x)}{dx}=u(x)$$\n",
        "\n",
        "Using an explicit Runge-Kutta method(RK45)→ JAX's odeint functiom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y6-OyfHJl0-7"
      },
      "outputs": [],
      "source": [
        "# Output sensor locations and measurements\n",
        "y_train = random.uniform(key_train, (P_train*100,)).sort() \n",
        "s_train = odeint(u_fn, 0.0, y_train) # Obtain the ODE solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "kTiLiL5gK917",
        "outputId": "6480d812-f687-4f8e-d762-e31d8b67204c"
      },
      "outputs": [],
      "source": [
        "plot_us(x,u,y_train,s_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BJmeBjxfjjg"
      },
      "source": [
        "Now, we will have to create many functions for our testing and training dataset, so let's create a pair of programing-functions to generate one random function at a time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS4wnXKpzHic"
      },
      "outputs": [],
      "source": [
        "# Geneate training data corresponding to one input sample\n",
        "def generate_one_training_data(key, m=100, P=1):\n",
        "    # Sample GP prior at a fine grid\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(0, 1, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(key, (N,)))\n",
        "\n",
        "    # Create a callable interpolation function  \n",
        "    u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "\n",
        "    # Input sensor locations and measurements\n",
        "    x = np.linspace(0, 1, m)\n",
        "    u = vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
        "\n",
        "    # Output sensor locations and measurements\n",
        "    y_train = random.uniform(key, (P,)).sort() \n",
        "    s_train = odeint(u_fn, 0.0, np.hstack((0.0, y_train)))[1:] # JAX has a bug and always returns s(0), so add a dummy entry to y and return s[1:]\n",
        "\n",
        "    # Tile inputs\n",
        "    u_train = np.tile(u, (P,1))\n",
        "\n",
        "    return u_train, y_train, s_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bhn_Dy-FWph0"
      },
      "outputs": [],
      "source": [
        "# Geneate test data corresponding to one input sample\n",
        "def generate_one_test_data(key, m=100, P=100):\n",
        "    # Sample GP prior at a fine grid\n",
        "    N = 512\n",
        "    gp_params = (1.0, length_scale)\n",
        "    jitter = 1e-10\n",
        "    X = np.linspace(0, 1, N)[:,None]\n",
        "    K = RBF(X, X, gp_params)\n",
        "    L = np.linalg.cholesky(K + jitter*np.eye(N))\n",
        "    gp_sample = np.dot(L, random.normal(key, (N,)))\n",
        "\n",
        "    # Create a callable interpolation function  \n",
        "    u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "\n",
        "    # Input sensor locations and measurements\n",
        "    x = np.linspace(0, 1, m)\n",
        "    u = vmap(u_fn, in_axes=(None,0))(0.0, x)\n",
        "\n",
        "    # Output sensor locations and measurements\n",
        "    y = np.linspace(0, 1, P)\n",
        "    s = odeint(u_fn, 0.0, y)\n",
        "\n",
        "    # Tile inputs\n",
        "    u = np.tile(u, (P,1))\n",
        "\n",
        "    return u, y, s "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx2GnFcxy2vq"
      },
      "source": [
        "## Data Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKqeYeFVbmEJ"
      },
      "outputs": [],
      "source": [
        "# Training Data\n",
        "N_train = 10000 #Number of functions\n",
        "m = 100 # number of input sensors\n",
        "P_train = 1   # number of output sensors\n",
        "key_train = random.PRNGKey(0)  # use different key for generating training data and test data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LctcnSb_hVr2"
      },
      "outputs": [],
      "source": [
        "config.update(\"jax_enable_x64\", True) # Enable double precision\n",
        "keys = random.split(key_train, N_train) # Obtain 10000 random numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZCfAP7Qha8N"
      },
      "outputs": [],
      "source": [
        "gen_fn = jit(lambda key: generate_one_training_data(key, m, P_train)) #lets call our function\n",
        "u_train, y_train, s_train = vmap(gen_fn)(keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrIKONnAiLBs"
      },
      "outputs": [],
      "source": [
        "# Reshape the data\n",
        "u_train = np.float32(u_train.reshape(N_train * P_train,-1))\n",
        "y_train = np.float32(y_train.reshape(N_train * P_train,-1))\n",
        "s_train = np.float32(s_train.reshape(N_train * P_train,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t72zeQnJirdA"
      },
      "outputs": [],
      "source": [
        "# Testing Data\n",
        "N_test = 1 # number of input samples \n",
        "P_test = m   # number of sensors \n",
        "key_test = random.PRNGKey(12345) # A different key "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfTydf5Cjkrh"
      },
      "outputs": [],
      "source": [
        "keys = random.split(key_test, N_test)\n",
        "gen_fn = jit(lambda key: generate_one_test_data(key, m, P_test))\n",
        "u, y, s = vmap(gen_fn)(keys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_cXqtdFjo57"
      },
      "outputs": [],
      "source": [
        "#Reshape the data\n",
        "u_test = np.float32(u.reshape(N_test * P_test,-1))\n",
        "y_test = np.float32(y.reshape(N_test * P_test,-1))\n",
        "s_test = np.float32(s.reshape(N_test * P_test,-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm6kLyH0lev8"
      },
      "outputs": [],
      "source": [
        "# Data generator\n",
        "class DataGenerator(data.Dataset):\n",
        "    def __init__(self, u, y, s, \n",
        "                 batch_size=64, rng_key=random.PRNGKey(1234)):\n",
        "        'Initialization'\n",
        "        self.u = u # input sample\n",
        "        self.y = y # location\n",
        "        self.s = s # labeled data evulated at y (solution measurements, BC/IC conditions, etc.)\n",
        "        \n",
        "        self.N = u.shape[0]\n",
        "        self.batch_size = batch_size\n",
        "        self.key = rng_key\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        self.key, subkey = random.split(self.key)\n",
        "        inputs, outputs = self.__data_generation(subkey)\n",
        "        return inputs, outputs\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def __data_generation(self, key):\n",
        "        'Generates data containing batch_size samples'\n",
        "        idx = random.choice(key, self.N, (self.batch_size,), replace=False)\n",
        "        s = self.s[idx,:]\n",
        "        y = self.y[idx,:]\n",
        "        u = self.u[idx,:]\n",
        "        # Construct batch\n",
        "        inputs = (u, y)\n",
        "        outputs = s\n",
        "        return inputs, outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfgoQuM6lKTa"
      },
      "source": [
        "# DeepOnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jslem3iHlwAs"
      },
      "outputs": [],
      "source": [
        "# Define the neural net\n",
        "def MLP(layers, activation=relu):\n",
        "  ''' Vanilla MLP'''\n",
        "  def init(rng_key):\n",
        "      \n",
        "      return params\n",
        "  def apply(params, inputs):\n",
        "      \n",
        "      return outputs\n",
        "  return init, apply"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ZG1bVnij-bg"
      },
      "outputs": [],
      "source": [
        "# Define the model\n",
        "class DeepONet:\n",
        "    def __init__(self, branch_layers, trunk_layers):    \n",
        "        # Network initialization and evaluation functions\n",
        "        \n",
        "\n",
        "        # Initialize\n",
        "        \n",
        "\n",
        "        # Use optimizers to set optimizer initialization and update functions\n",
        "       \n",
        "        # Logger\n",
        "       \n",
        "\n",
        "    # Define opeartor net\n",
        "    def operator_net(self, params, u, y):\n",
        "        \n",
        "      \n",
        "    # Define ODE/PDE residual\n",
        "    def residual_net(self, params, u, y):\n",
        "        \n",
        "\n",
        "    # Define loss\n",
        "    def loss(self, params, batch):\n",
        "        # Fetch data\n",
        "        # inputs: (u, y), shape = (N, m), (N,1)\n",
        "        # outputs: s, shape = (N,1)\n",
        "\n",
        "\n",
        "        # Compute forward pass\n",
        "\n",
        "        # Compute loss\n",
        "\n",
        "        return loss\n",
        "\n",
        "    # Define a compiled update step\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def step(self, i, opt_state, batch):\n",
        "        params = self.get_params(opt_state)\n",
        "        g = grad(self.loss)(params, batch)\n",
        "        return self.opt_update(i, g, opt_state)\n",
        "\n",
        "    # Optimize parameters in a loop\n",
        "    def train(self, dataset, nIter = 10000):\n",
        "        data = iter(dataset)\n",
        "        pbar = trange(nIter)\n",
        "        # Main training loop\n",
        "        for it in pbar:\n",
        "            batch = next(data)\n",
        "            self.opt_state = self.step(next(self.itercount), self.opt_state, batch)\n",
        "            \n",
        "            if it % 100 == 0:\n",
        "                params = self.get_params(self.opt_state)\n",
        "\n",
        "                # Compute loss\n",
        "                loss_value = self.loss(params, batch)\n",
        "\n",
        "                # Store loss\n",
        "                self.loss_log.append(loss_value)\n",
        "\n",
        "                # Print loss during training\n",
        "                pbar.set_postfix({'Loss': loss_value})\n",
        "       \n",
        "    # Evaluates predictions at test points  \n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def predict_s(self, params, U_star, Y_star):\n",
        "        s_pred = vmap(self.operator_net, (None, 0, 0))(params, U_star, Y_star)\n",
        "        return s_pred\n",
        "\n",
        "    @partial(jit, static_argnums=(0,))\n",
        "    def predict_s_y(self, params, U_star, Y_star):\n",
        "        s_y_pred = vmap(self.residual_net, (None, 0, 0))(params, U_star, Y_star)\n",
        "        return s_y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1bmvbf1rTx2"
      },
      "source": [
        "# Evaluate our Operator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5RUVLmalT4P"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "# For vanilla DeepONet, shallower network yields better accuarcy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrXXP3ztlun6"
      },
      "outputs": [],
      "source": [
        "# Create data set\n",
        "batch_size = 10000\n",
        "dataset = DataGenerator(u_train, y_train, s_train, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUDa7cAdl9wR",
        "outputId": "90d38e9a-379c-409e-8e74-935ee0ddce20"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eXwnbxoQmR5w"
      },
      "outputs": [],
      "source": [
        "# Predict\n",
        "params = model.get_params(model.opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nO5cCL6mV-F"
      },
      "outputs": [],
      "source": [
        "s_pred = model.predict_s(params, u_test, y_test)[:,None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NelrdvVvmXPN"
      },
      "outputs": [],
      "source": [
        "s_y_pred = model.predict_s_y(params, u_test, y_test) # remember that s_y=ds/dy=u"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-l8xC6cmjXv"
      },
      "outputs": [],
      "source": [
        "# Compute relative l2 error\n",
        "error_s = np.linalg.norm(s_test - s_pred) / np.linalg.norm(s_test) \n",
        "error_u = np.linalg.norm(u_test[::P_test].flatten()[:,None] - s_y_pred) / np.linalg.norm(u_test[::P_test].flatten()[:,None]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3OBppzUqMsa",
        "outputId": "5e3745ad-6d82-4d42-f33a-c5953fe911a9"
      },
      "outputs": [],
      "source": [
        "print(error_s,error_u)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os7XSd21Xj3l"
      },
      "source": [
        "## Visualize the results for the first function in our Testing Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxObn9b8sNFI"
      },
      "outputs": [],
      "source": [
        "idx=0\n",
        "index = np.arange(idx * P_test,(idx + 1) * P_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "IJs6uuFKsZav",
        "outputId": "2e60e6f8-6214-4a3e-d75a-646909c2d8a8"
      },
      "outputs": [],
      "source": [
        "# Compute the relative l2 error for one input sample \n",
        "error_u = np.linalg.norm(s_test[index, :] - s_pred[index, :], 2) / np.linalg.norm(s_test[index, :], 2) \n",
        "error_s = np.linalg.norm(u_test[::P_test][idx].flatten()[:,None] - s_y_pred[index, :], 2) / np.linalg.norm(u_test[::P_test][idx].flatten()[:,None], 2) \n",
        "\n",
        "print(\"error_u: {:.3e}\".format(error_u))\n",
        "print(\"error_s: {:.3e}\".format(error_s))\n",
        "\n",
        "# Visualizations\n",
        "# Predicted solution s(y)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(y_test[index, :], s_test[index, :], label='Exact s', lw=2)\n",
        "plt.plot(y_test[index, :], s_pred[index, :], '--', label='Predicted s', lw=2)\n",
        "plt.xlabel('y')\n",
        "plt.ylabel('s(y)')\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(y_test[index, :], s_pred[index, :] - s_test[index, :], '--', lw=2, label='error')\n",
        "plt.tight_layout()\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Predicted residual u(x)\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(y_test[index, :], u_test[::P_test][idx], label='Exact u', lw=2)\n",
        "plt.plot(y_test[index, :], s_y_pred[index,:], '--', label='Predicted u', lw=2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(y_test[index, :], s_y_pred[index,:].flatten() - u_test[::P_test][idx] , '--', label='error', lw=2)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2Tj3ENYAS1f"
      },
      "source": [
        "# Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9D9TlZpXvVS"
      },
      "source": [
        "Lets obtain the anti-derivative of a trigonometric function. **However**, remember that this neural operator works for $x\\in[0,1]$ when the antiderivative's initial value ($s(0)=0$). To fulfill that conditions, we will use $u(x)=cos(2\\pi x),∀x\\in[0,1]$.\n",
        "\n",
        "\n",
        "So, we will evaluate our operator ($G$):\n",
        "\n",
        "\n",
        "$$G:u(x)→s(x)=s(0)+\\int_{0}^{x}u(t)dt$$\n",
        "\n",
        "to $u(t)=cos(2\\pi t)$:\n",
        "\n",
        "$$s(x)=s(0)+\\int_{0}^{x}cos(2\\pi t)dt$$\n",
        "\n",
        "Since $s(0)=0$, the answer would be (the integral of u):\n",
        "\n",
        "$$s(x)=\\frac{1}{2\\pi}sin(2\\pi x)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_us(x,u,y,s):\n",
        "  fig, ax1 = plt.subplots(figsize=(8, 6))\n",
        "  plt.rcParams['font.size'] = '18'\n",
        "  color='black'\n",
        "  wdt=1.5\n",
        "  ax1.plot(x,u,'k--',label='$u(x)=ds/dx$',linewidth=wdt)\n",
        "  ax1.plot(y,s,'o-',label='$s(x)=s(0)+\\int u(t)dt|_{t=y}$',linewidth=wdt)\n",
        "  ax1.set_xlabel('x',fontsize='large')\n",
        "  ax1.set_ylabel('u',fontsize='large')\n",
        "  ax1.tick_params(axis='y', color=color)\n",
        "  ax1.legend(loc = 'lower right', ncol=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBbYSKUGAJU1"
      },
      "outputs": [],
      "source": [
        "#u_fn = lambda x, t: np.interp(t, X.flatten(), gp_sample)\n",
        "u_fn = lambda t, x: -np.cos(2*np.pi*x)\n",
        "# Input sensor locations and measurements\n",
        "x = np.linspace(0, 1, m)\n",
        "u = u_fn(None,x)\n",
        "# Output sensor locations and measurements\n",
        "y =random.uniform(key_train, (m,)).sort()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FrbAxKkALIi"
      },
      "outputs": [],
      "source": [
        "# reshapte the data to be processed by our DeepOnet\n",
        "u2=np.tile(u,100)\n",
        "u2=np.float32(u2.reshape(N_test * P_test,-1))\n",
        "y=y.reshape(len(y),1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg6jY2IbANDO"
      },
      "outputs": [],
      "source": [
        "s=model.predict_s(params, u2, y)[:,None]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "J13kyBSwAO9X",
        "outputId": "53eb8cb2-da7c-43c3-f447-bae0c9b2e9c3"
      },
      "outputs": [],
      "source": [
        "plot_us(x,u,y,s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAmGDh9OfSQd"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Lu, L., Jin, P., & Karniadakis, G. E. (2019). Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators. arXiv preprint arXiv:1910.03193.\n",
        "\n",
        "[2] Wang, S., Wang, H., & Perdikaris, P. (2021). Learning the solution operator of parametric partial differential equations with physics-informed DeepONets. Science advances, 7(40), eabi8605.\n",
        "\n",
        "[3] Ray, D., Pinti, O., & Oberai, A. A. (2023). Deep Learning and Computational Physics (Lecture Notes). arXiv preprint arXiv:2301.00942."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "7. Antiderivative(DeepOnets).ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
