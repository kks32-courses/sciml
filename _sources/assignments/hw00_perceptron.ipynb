{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI7P5CAfzv7Y"
      },
      "source": [
        "# HW 00: Perceptron"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i6jOIIvjy5cv"
      },
      "source": [
        "# Regularization Techniques: L1 and L2 Regularization\n",
        "\n",
        "Regularization is a crucial concept in machine learning, ensuring that models generalize well to new, unseen data. Among the commonly used regularization methods are L1 and L2 regularizations. This document outlines their concepts, applications, and key differences.\n",
        "\n",
        "## L2 Regularization (Ridge Regularization)\n",
        "\n",
        "L2 regularization, also known as Ridge or Tikhonov regularization, combats overfitting, particularly prevalent in linear regression and neural networks.\n",
        "\n",
        "### Key Features of L2 Regularization:\n",
        "\n",
        "- **Penalizing Large Coefficients:**\n",
        "  L2 regularization penalizes the model for having large weights. This discourages over-reliance on any single feature and thereby guards against overfitting.\n",
        "\n",
        "- **Mathematical Formulation:**\n",
        "  L2 regularization augments the original loss function by adding the squared magnitude of the weights. In mathematical terms:\n",
        "  $$\n",
        "  L_{regularized} = L + \\alpha \\sum_i w_i^2\n",
        "  $$\n",
        "  Where:\n",
        "  - $ L_{regularized} $ is the regularized loss.\n",
        "  - $ \\alpha $ is the regularization strength.\n",
        "  - $ w_i $ represents each weight in the model.\n",
        "\n",
        "### Benefits:\n",
        "\n",
        "- Prevents overfitting by constraining the magnitude of the weights.\n",
        "- Leads to more stable models, especially when features are correlated.\n",
        "- Encourages smaller weight values, making potentially smoother models.\n",
        "\n",
        "## L1 Regularization (Lasso Regularization)\n",
        "\n",
        "L1 regularization, or Lasso regularization, is another technique that prevents overfitting. Unlike L2, which penalizes the square of the weights, L1 penalizes the absolute value of the weights.\n",
        "\n",
        "### Mathematical Formulation:\n",
        "L1 regularization is mathematically expressed as:\n",
        "$$\n",
        "L_{regularized} = L + \\alpha \\sum_i |w_i|\n",
        "$$\n",
        "\n",
        "### Key Difference:\n",
        "\n",
        "- While L2 regularization tends to make weights small but non-zero, L1 can make some weights exactly zero, leading to sparse models.\n",
        "\n",
        "\n",
        "### Hint:\n",
        "\n",
        "You get the model weights by using\n",
        "\n",
        "```python\n",
        "p in model.parameters\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1.1: Implement L1 and L2 regularization and explain the effect of regularization. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "D4nfoxhTtKro",
        "outputId": "dee35bfb-e890-4b6f-9cd8-c9d7c157fbf6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.datasets import make_moons, make_blobs\n",
        "\n",
        "\n",
        "np.random.seed(1337)\n",
        "random.seed(1337)\n",
        "\n",
        "# make up a dataset\n",
        "X, y = make_moons(n_samples=100, noise=0.1)\n",
        "y = y*2 - 1  # make y be -1 or 1\n",
        "\n",
        "# visualize in 2D\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n",
        "\n",
        "# Model definition in PyTorch using nn.Sequential\n",
        "## Create a PyTorch NN with two layers and 16 neurons in each layer\n",
        "print(model)\n",
        "\n",
        "# loss function\n",
        "def loss(batch_size=None):\n",
        "    if batch_size is None:\n",
        "        Xb, yb = X, y\n",
        "    else:\n",
        "        ri = np.random.permutation(X.shape[0])[:batch_size]\n",
        "        Xb, yb = X[ri], y[ri]\n",
        "\n",
        "    inputs = torch.tensor(Xb, dtype=torch.float32)\n",
        "    labels = torch.tensor(yb, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # forward the model to get scores\n",
        "    scores = model(inputs)\n",
        "\n",
        "    # svm \"max-margin\" loss\n",
        "    losses = F.relu(1 + -labels*scores)\n",
        "    data_loss = losses.mean()\n",
        "\n",
        "    # Loss\n",
        "    ### Implement L1 and L2 regularization here\n",
        "    total_loss = data_loss\n",
        "\n",
        "    # also get accuracy\n",
        "    accuracy = ((labels > 0) == (scores > 0)).float().mean()\n",
        "    return total_loss, accuracy\n",
        "\n",
        "total_loss, acc = loss()\n",
        "print(total_loss.item(), acc.item())\n",
        "\n",
        "# optimization\n",
        "for k in range(100):\n",
        "    # forward\n",
        "    total_loss, acc = loss()\n",
        "\n",
        "    # backward\n",
        "    model.zero_grad()\n",
        "    total_loss.backward()\n",
        "\n",
        "    # update (sgd)\n",
        "    learning_rate = 1.0\n",
        "    with torch.no_grad():\n",
        "        for p in model.parameters():\n",
        "            p -= learning_rate * p.grad\n",
        "\n",
        "    if k % 10 == 0:\n",
        "        print(f\"step {k} loss {total_loss.item()}, accuracy {acc*100}%\")\n",
        "\n",
        "\n",
        "# visualize decision boundary\n",
        "h = 0.25\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Xmesh = np.c_[xx.ravel(), yy.ravel()]\n",
        "with torch.no_grad():\n",
        "    scores = model(torch.tensor(Xmesh, dtype=torch.float32))\n",
        "Z = (scores > 0).numpy().reshape(xx.shape)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n",
        "plt.xlim(xx.min(), xx.max())\n",
        "plt.ylim(yy.min(), yy.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q1.2: \n",
        "\n",
        "I use a constant learning rate of 1.0. See the effect of LR by exponentially decaying LR with each iteration reaching a value of 0.1 at the end of the iterations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Q2: NN Example 3 - Herschel Bulkley model\n",
        "\n",
        "Modify the hyperparameters such as number of layers, neurons, regularization, learning rate, etc and evaluate its effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate data\n",
        "num_samples = 10000\n",
        "v_values = torch.rand(num_samples, 1) * 10\n",
        "tau0_values = torch.rand(num_samples, 1) * 5\n",
        "k_values = torch.rand(num_samples, 1) * 2\n",
        "n_values = torch.rand(num_samples, 1) * 0.5 + 0.5 # 0.5 - 1.0\n",
        "\n",
        "# Inputs\n",
        "inputs = torch.cat([v_values, tau0_values, k_values, n_values], dim=1)\n",
        "\n",
        "# Targets\n",
        "y_true = tau0_values + k_values * (v_values**n_values)\n",
        "\n",
        "# Model\n",
        "model = nn.Sequential(\n",
        "   nn.Linear(4, 64),\n",
        "   nn.Tanh(),\n",
        "   nn.Linear(64,1)\n",
        ")\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "epochs = 5000\n",
        "for epoch in range(epochs):\n",
        "   y_pred = model(inputs)\n",
        "   loss = loss_fn(y_pred, y_true)\n",
        "\n",
        "   optimizer.zero_grad()\n",
        "   loss.backward()\n",
        "   optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "v_eval = torch.linspace(0, 10, 100).unsqueeze(1)\n",
        "tau0_eval = 5 * torch.ones_like(v_eval)\n",
        "k_eval = 2 * torch.ones_like(v_eval)\n",
        "n_eval = 0.7 * torch.ones_like(v_eval)\n",
        "\n",
        "inputs_eval = torch.cat([v_eval, tau0_eval, k_eval, n_eval], dim=1)\n",
        "y_pred = model(inputs_eval)\n",
        "\n",
        "y_true_eval = tau0_eval + k_eval * (v_eval**n_eval)\n",
        "\n",
        "# Plot\n",
        "plt.plot(v_eval.numpy(), y_true_eval.numpy(), 'r-', label='True')\n",
        "plt.plot(v_eval.numpy(), y_pred.detach().numpy(), 'b.', label='Predicted')\n",
        "plt.xlabel('Shear Rate')\n",
        "plt.ylabel('Shear Stress')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
